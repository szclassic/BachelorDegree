{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# library to let the user interact with the OS that the python is running on\nimport os\n# importing library to help with data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\n# importing library to help with data analysis and manipulation\nimport numpy as np\n# a collcetion of command style functions that makes changes to figures and plots\nimport matplotlib.pyplot as plt\n# a function in inline-mode that allows for the display of generated plots\n%matplotlib inline\n# importing the 'seaborn' library that provides a interface for creating informative graphs\nimport seaborn as sns\n# importing the Regular Expression syntax operations\nimport re\n# importing the nltk library\nimport nltk\n# function to stop word removal so that the nltk library won't change the frequency of words\nfrom nltk.corpus import stopwords\n# function to remove affixes from the word, and returns the word stem\nfrom nltk.stem.porter import PorterStemmer\n# function to return the input word\nfrom nltk.stem import WordNetLemmatizer\n# 'word_tokenize': function that splits a sentence into words\n# 'sent_tokenize': function that tokenizes inserted text into sentences\nfrom nltk.tokenize import word_tokenize,sent_tokenize\n# function to convert text documents into matrix of token counts\nfrom sklearn.feature_extraction.text import CountVectorizer\n# 'train_test_split': function to split matrices into train and test sets\n# 'cross_val_score': function to evaluate a score by cross-validation\nfrom sklearn.model_selection import train_test_split, cross_val_score\n# importing 'LogisticRegression' that allows us to perfrom machine learning with linear models\nfrom sklearn.linear_model import LogisticRegression\n# importing 'classification_report' that allows for metric report to measure classification performance\nfrom sklearn.metrics import classification_report\n# importing 'GridSearchCV' that finds the optimal value from the set that it is called upon\nfrom sklearn.model_selection import GridSearchCV\n\n# implementing the train set\ntrain = '../input/covid-19-nlp-text-classification/Corona_NLP_train.csv'\n# implementing the test set\ntest = '../input/covid-19-nlp-text-classification/Corona_NLP_test.csv'\n\n# making a copy of the train set to retain the initial structure\ntrainOriginal = pd.read_csv(train, encoding='latin-1')\n# making a copy of the test set to retain the initial structure\ntestOriginal = pd.read_csv(test, encoding='latin-1')\n\n# I believe that the purpose of the next two coded lines are to preserve the state of the original sets\n# setting the content of the test set to the copied version\ntrain = trainOriginal.copy()\n# setting the content of the test set to the copied version\ntest = testOriginal.copy()\n\n# Earlier attempts to open the files with a utf-8 encoding lead to a unicode error as it couldn't\n# parse certain parts of the file, hence, utf-8 was introduced as a solution.\n\n# going to the beginning of the train set\ntrain.head()\n# going to the beginning of the test set\ntest.head()\n\n# The datasets contain 7 columns housing the data info. The UserName and ScreenName has being\n# encrypted due to privacy concerns. The tweets contains mentions and hashtags which must be cleaned\n# in order to help the models better understand the statistical relationship between the relevant\n# details. The sentiment column contains 5 different classes which can be remapped into 3 for better\n# statistical understanding. The other columns are the timeframe of the tweets and the location from\n# where the tweets where twitted.\n\n# displaying information about the train set (such as columns, data types, and memory usage)\ntrain.info()\n\n# I believe that this line returns the number of missing values in the dataset\ntrain.isnull().sum()\n\n# The location column contains a whooping 8590 missing rows. Filling the blanks with the most common\n# location won't really make sense as the missing details are too much.\ntrain['Location'].value_counts()[:60]\n\n# splitting location into word pairs in the train set\ntrain['Location'] = train['Location'].str.split(\",\").str[0]\n# splitting location into word pairs in the test set\ntest['Location'] = test['Location'].str.split(\",\").str[0]\n\n# displaying the given parameters in the train set that appear at least 60 times\ntrain['Location'].value_counts()[:60]\ntrain['TweetAt'].value_counts()\n\n# The data collected was tweeted between 16th March, 2020 to 14th April, 2020. Any model built and \n# deployed at this time may likely not be relevant for present use due to new findings, researches, \n# tresnd that have emerged which will influence every recent covid19 related tweets. Any model built\n# using this data will be a decayed model and further decay will happen at a rapid pace.\ntrain['Sentiment'].value_counts()\n\n# setting the width and height of the plot\nplt.figure(figsize=(10,10))\n# creating a histogram using '.countplot' function\nsns.countplot(y='Location',data=train,order=train.Location.value_counts().iloc[\n    0:19].index).set_title(\"Twitted locations\")\n# setting the formatting of the histogram\nsns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(11,8)})\nsns.countplot(train['Sentiment'])\n\n# creating the name of the labels for the pie chart\nlabels = ['Positve', 'Negative', 'Neutral', 'Extremely Positive', 'Extremely Negative']\n# setting the colors of each bar in the pie chart\ncolors = ['#ff9999','#66b3ff','#99ff99','#ffcc99', '#ff5645']\nexplode = (0.05,0.05,0.05,0.05,0.05) \n# plotting the pie chart\nplt.pie(train.Sentiment.value_counts(), colors = colors, labels=labels,\n        autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\n# setting the center of the circle in the pie chart and coloring it white\ncentreCircle = plt.Circle((0,0),0.70,fc='white')\n# getting the current figure in order to get the appropriate axis information\nfig = plt.gcf()\nfig.gca().add_artist(centreCircle)\n# adjusting the parameters of the subplot\nplt.tight_layout()\nplt.show()\n\n# locating the the data from the train set\nplotDf = train.iloc[:,[2,5]] #[:,[2,5]] is the location and sentiment columns\n# displaying the content\nplotDf\n\n# setting the size of the chart\nsns.set(rc={'figure.figsize':(15,9)})\n# calling upon the train set to fetch data\ngg = train.Location.value_counts()[:5].index\n# giving a title to the graph and setting its font-size\nplt.title('Sentiment Categories of the First 5 Top Locations', fontsize=16, fontweight='bold')\n# plotting the graph\nsns.countplot(x = 'Location', hue = 'Sentiment', data = plotDf, order = gg)\n\n# setting the identity value of 0. to the train set\ntrain['Identity'] = 0\n# setting the identity value of 1. to the test set\ntest['Identity'] = 1 \n# concatenating the test and the train set\ncovid = pd.concat([train, test])\ncovid.reset_index(drop=True, inplace=True)\n# going to the beginning of the now concatenated sets\ncovid.head()\ncovid['Sentiment'] = covid['Sentiment'].str.replace('Extremely Positive', 'Positive')\ncovid['Sentiment'] = covid['Sentiment'].str.replace('Extremely Negative', 'Negative')\n\n# dropping the screen\ncovid = covid.drop('ScreenName', axis=1)\n# dropping the username\ncovid = covid.drop('UserName', axis=1)\n# displaying the newly set\ncovid\n\n# in the next few coded lines, we are setting the formatting for plotting the dataset\nsns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(11,8)})\nsns.countplot(covid['Sentiment'])\n# setting the name of the labels\nlabels = ['Positve', 'Negative', 'Neutral']\n# setting the color of the bars\ncolors = ['lightblue','lightsteelblue','silver']\nexplode = (0.1, 0.1, 0.1)\n# displaying the pie chart\nplt.pie(covid.Sentiment.value_counts(), colors = colors, labels=labels,\n        shadow=300, autopct='%1.1f%%', startangle=90, explode = explode)\nplt.show()\n\n# setting the width and height of the plot\nplt.figure(figsize=(10,10))\n# creating the graph\nsns.countplot(y='Location',data=train,order=train.Location.value_counts().iloc[\n    0:19].index).set_title(\"Twitted locations\")\ncovid['Sentiment'] = covid['Sentiment'].map({'Neutral':0, 'Positive':1, 'Negative':2})\n# processing and analyzing the data given the parameters\nhashTags=covid['OriginalTweet'].str.extractall(r\"(#\\S+)\")\nhashTags = hashTags[0].value_counts()\nhashTags[:50]\nmentions = train['OriginalTweet'].str.extractall(r\"(@\\S+)\")\nmentions = mentions[0].value_counts()\nmentions[:50]\n\n# creating a function that removes hashtags, urls, mentions, digits, and stopwords\ndef clean(text):\n    text = re.sub(r'http\\S+', \" \", text)\n    text = re.sub(r'@\\w+',' ',text)\n    text = re.sub(r'#\\w+', ' ', text)\n    text = re.sub(r'\\d+', ' ', text)\n    text = re.sub('r<.*?>',' ', text)\n    text = text.split()\n    text = \" \".join([word for word in text if not word in stopWord])\n    \n    return text\n\n# finding the common words in the english language and going to the beginning of the data set\nstopWord = stopwords.words('english')\ncovid['OriginalTweet'] = covid['OriginalTweet'].apply(lambda x: clean(x))\ncovid.head()\ncovid = covid[['OriginalTweet','Sentiment','Identity']]\ncovid.head()\n\n# tokenizing the words inside of the dataset\ncovid['Corpus'] = [nltk.word_tokenize(text) for text in covid.OriginalTweet]\nlemma = nltk.WordNetLemmatizer()\n# assigning the tokenized words into the corpus\ncovid.Corpus = covid.apply(lambda x: [lemma.lemmatize(word) for word in x.Corpus], axis=1)\ncovid.Corpus = covid.apply(lambda x: \" \".join(x.Corpus),axis=1)\n# going back to the beginning of the dataset\ncovid.head()\n\n# splitting the data set to the train set\ntrain = covid[covid.Identity==0]\n# splitting the data set to the test set\ntest = covid[covid.Identity==1]\n# dropping values from both the test and the train set\ntrain.drop('Identity',axis=1, inplace=True)\ntest.drop('Identity',axis=1, inplace=True)\ntest.reset_index(drop=True,inplace=True)\n# going to the beginning of the train set\ntrain.head()\n# going to the beginning of the test set\ntest.head()\n\n# splitting the train set into two separate sets\nXTrain = train.Corpus\nyTrain = train.Sentiment\n# splitting the test set into two separate sets\nXTest = test.Corpus\nyTest = test.Sentiment\n\n# getting a validation test for the train set\nXTrain, XVal, yTrain, yVal = train_test_split(XTrain, yTrain, test_size=0.2,random_state=42)\nXTrain.shape, XVal.shape, yTrain.shape, yVal.shape, XTest.shape, yTest.shape\n\n# transforming the texts into the numerical values\nvectorizer = CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=5).fit(covid.Corpus)\nXTrainVec = vectorizer.transform(XTrain)\nXValVec = vectorizer.transform(XVal)\nXTestVec = vectorizer.transform(XTest)\n# running a logistic regression\nlogReg = LogisticRegression(random_state=42)\n# seeing how accurate the logistic regression is\ncross_val_score(LogisticRegression(random_state=42),\n                XTrainVec, yTrain, cv=10, verbose=1, n_jobs=-1).mean()\nmodel = logReg.fit(XTrainVec, yTrain)\n# displaying the information \nprint(classification_report(yVal, model.predict(XValVec)))\n\n# finding and returning the number spaces on a log scale\npenalty = ['l2']\nC = np.logspace(0, 4, 10)\nhyperparameters = dict(C=C, penalty=penalty)\n# using the 'GridSearchCV' to fine tune the logistic regression model\nlogRegGrid = GridSearchCV(logReg, hyperparameters, cv=5, verbose=0)\nbestModel = logRegGrid.fit(XTrainVec, yTrain)\n\n# displaying the best hyperparameters combination\nprint('Best Penalty:', bestModel.best_estimator_.get_params()['penalty'])\nprint('Best C:', bestModel.best_estimator_.get_params()['C'])\n\n# Final Logistic Regression model performance\nyPred = bestModel.predict(XTestVec)\n# displaying the model performance\nprint(classification_report(yTest, bestModel.predict(XTestVec)))\n\n# INPUT PARAMETERS:\n# Throughout this assignment, by reading the provided file I believe that the input parameters\n# are given to be integrated with the data and information taken from the test and the train set.\n# A classifer would then utilize the data to see how the inut variables relate to the dataset.\n\n# CountVectorizer() function:\n# the 'CountVectorizer' function in this assignment is meant to transform a collection of texts\n# into numerical matrix of word. I believe that in this assignment specifically this is due to the fact that\n# we plotted multiple differnt graphs, and we need numerical data value in order to successfully plot a graph.\n\n# I was very unfamiliar with a lot of libraries and functions used in this assignment, so I used the following \n# resources in order to better understand certain parts:\n# https://www.geeksforgeeks.org/how-to-use-matplotlib-plot-inline/\n# https://pypi.org/project/matplotlib-inline/\n# https://matplotlib.org/2.0.2/users/pyplot_tutorial.html\n# https://www.nltk.org/api/nltk.tokenize.html\n# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n# https://seaborn.pydata.org/tutorial/introduction.html","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}